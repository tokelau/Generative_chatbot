{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","authorship_tag":"ABX9TyPeWnxVvcvTPWGA8m0dtokb"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive/')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jZG9h8vsR3w_","executionInfo":{"status":"ok","timestamp":1742670956351,"user_tz":-480,"elapsed":28756,"user":{"displayName":"Nastya","userId":"11252779665510215149"}},"outputId":"34951e7b-5025-47fd-b880-43b8713e7e9b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","source":["# Загружаем модель"],"metadata":{"id":"3zzMr87ZQ-57"}},{"cell_type":"code","source":["%%capture\n","!pip install bitsandbytes gradio"],"metadata":{"id":"o6XS9jxVQ5Sl","executionInfo":{"status":"ok","timestamp":1742671050001,"user_tz":-480,"elapsed":93652,"user":{"displayName":"Nastya","userId":"11252779665510215149"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n","from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n","import torch\n","\n","# Загружаем токенизатор и модель\n","model_name = \"Qwen/Qwen2.5-0.5B-Instruct\"\n","tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n","\n","finetuned_model = AutoModelForCausalLM.from_pretrained(\n","    model_name,\n","    load_in_4bit=True,\n","    device_map=\"auto\",\n","    torch_dtype=torch.float16\n",")\n","\n","# 3. Готовим модель к дообучению с LoRA\n","finetuned_model = prepare_model_for_kbit_training(finetuned_model)\n","\n","lora_config = LoraConfig(\n","    r=8,\n","    lora_alpha=16,\n","    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n","    lora_dropout=0.05,\n","    bias=\"none\",\n","    task_type=\"CAUSAL_LM\"\n",")\n","\n","finetuned_model = get_peft_model(finetuned_model, lora_config)\n","finetuned_model.load_state_dict(torch.load('/content/drive/MyDrive/Генерация в NLP/hw_task_2/models/qwen_House_1.pth'), strict=False)\n","\n","finetuned_model.eval()\n","\n","# Функция для инференса\n","def get_model_answer(prompt):\n","    messages = [\n","        {\"role\": \"system\", \"content\": \"Answer like Dr.House\"},\n","        {\"role\": \"user\", \"content\": prompt}\n","    ]\n","    text = tokenizer.apply_chat_template(\n","        messages,\n","        tokenize=False,\n","        add_generation_prompt=True\n","    )\n","    model_inputs = tokenizer([text], return_tensors=\"pt\").to(finetuned_model.device)\n","\n","    generated_ids = finetuned_model.generate(\n","        **model_inputs,\n","        max_new_tokens=32\n","    )\n","    generated_ids = [\n","        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n","    ]\n","\n","    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n","    return response"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wzgNs0k-RCvE","executionInfo":{"status":"ok","timestamp":1742671554644,"user_tz":-480,"elapsed":8996,"user":{"displayName":"Nastya","userId":"11252779665510215149"}},"outputId":"da607dfa-6e1d-44ea-9d74-81e9d00cbd9c"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stderr","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]}]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":684},"id":"-qXchoezOj7P","executionInfo":{"status":"ok","timestamp":1742671671559,"user_tz":-480,"elapsed":2995,"user":{"displayName":"Nastya","userId":"11252779665510215149"}},"outputId":"6d9aa773-13b7-444f-9b97-a6ffaf872966"},"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/gradio/chat_interface.py:334: UserWarning: The 'tuples' format for chatbot messages is deprecated and will be removed in a future version of Gradio. Please set type='messages' instead, which uses openai-style 'role' and 'content' keys.\n","  self.chatbot = Chatbot(\n"]},{"output_type":"stream","name":"stdout","text":["Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n","\n","Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://d43d45aea7fceb5fc8.gradio.live\n","\n","This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://d43d45aea7fceb5fc8.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":[]},"metadata":{},"execution_count":12}],"source":["import gradio as gr\n","\n","def get_response(message, history):\n","    response = get_model_answer(message)\n","    return f'{response}'\n","\n","\n","demo = gr.ChatInterface(get_response,\n","                        title=\"Talk to Dr.House!\",\n","                        theme=\"soft\",)\n","\n","demo.launch()"]},{"cell_type":"code","source":[],"metadata":{"id":"spXs0I1KUBsG","executionInfo":{"status":"ok","timestamp":1742671671572,"user_tz":-480,"elapsed":11,"user":{"displayName":"Nastya","userId":"11252779665510215149"}}},"execution_count":12,"outputs":[]}]}